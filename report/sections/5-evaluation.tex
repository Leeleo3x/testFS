\section{Evaluation}
Since testFS is a toy file system, we did not run any end-to-end file system
benchmarks such as {\tt fio}, or Filebench~\cite{filebench-tarasov16}. As a
result, we do not make claims as to the performance improvement associated with
real workloads. Instead, the goal of our evaluation is to quantify the
potential performance benefit for adopting multi-threaded asynchronous I/O for
NVMe devices in production-ready user space file systems.

Overall, our experiments show that our asynchronous I/O write path performs up
to $1.5\times$ better on single file writes and up to $3\times$ better on
multi-file writes compared to a synchronous I/O write path.

\subsection{Experimental Methodology}
\paragraph{Setup.}
We ran our experiments on a machine\footnote{We used {\tt mel-12}.} with a
14-core Intel Xeon 2.40 GHz CPU and 128 GB of memory. The machine was equipped
with two Seagate PLC Nytro 5000 NVMe SSDs. We used one of these SSDs, with a
capcity of 400 GB, in our experiments. Our code was linked with the version of
SPDK at git hash
\href{https://github.com/spdk/spdk/commit/a2bf3cded37b7cc7e402eae80da90891f921b56d}{\tt a2bf3cded}.
We used a fixed block size of 512 bytes when interacting with the SSD.

\paragraph{Methodology.}
In each experiment we measured the amount of time it took to complete a given
task up to a granularity of microseconds. We repeated each experiment five
times and used the average of all five trials in our results. We used primitive
futures (described in Section~\ref{sec:futures}) as the thread synchronization
mechanism in all of our experiments.

\paragraph{Baseline.}
In our evaluations we want to quantify the performance improvement associated
with using asynchronous I/O requests with multiple threads on the testFS file
write path over synchronous I/O requests. Therefore our baseline is a
synchronous version of the testFS write path (each asynchronous I/O function
call is replaced with its synchronous counterpart). For a fair comparison, both
the synchronous and asynchronous write paths use our improved write path
implementation that we describe in Section~\ref{sec:writepath}.

\begin{figure}
  \centering
  \input{charts/raw-seq-speedup}
  \caption{The speedup of asynchronous I/O reads and writes for a varying
    number of blocks.}\label{fig:raw-speedup}
\end{figure}

\subsection{Raw Sequential Reads and Writes}
To quantify the performance improvement of purely asynchronous I/O versus
synchronous I/O, we used a microbenchmark that reads/writes a sequence of
blocks one by one. For each trial we read (or write) one block in a loop for a
number of iterations. In the synchronous case each read/write blocks until the
request completes. In the asynchronous case we issue all read/write requests
and then wait for them to complete.

Figure~\ref{fig:raw-speedup} shows our results. As the number of blocks
read/written increases, the speedup increases. Additionally the read speedup
(up to $7\times$) is more significant than the write speedup (up to $2\times$).
This is likely because reads are faster than writes in flash based SSDs, which
would mean that reads benefit more from asynchronous I/O requests since more
requests can be queued up at once.

\begin{figure*}
\centering
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/single-file-write-speedup}
  \caption{Single file write speedup}\label{fig:single-file-speedup}
\end{subfigure}\hfill%
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/single-file-write-thpt}
  \caption{Single file write effective throughput}\label{fig:single-file-thpt}
\end{subfigure}
\par\bigskip
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/multi-file-write-speedup}
  \caption{Multi-file write speedup}\label{fig:multi-file-speedup}
\end{subfigure}%
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/multi-file-write-thpt}
  \caption{Multi-file write effective throughput for 100 data blocks}
  \label{fig:multi-file-thpt}
\end{subfigure}
\caption{The speedup and effective write throughput when appending a varying
number of data blocks to a single file and when appending a fixed number of
blocks to a varying number of files.}
\end{figure*}

\subsection{File Writes}
In these experiments, we quantify the performance improvements associated with
multi-threaded asynchronous I/O when varying the number of blocks written to a
file and the number of files written in a single transaction. We report the
effective write throughput, which is calculated by dividing the total size of
the data blocks being written by the time taken to complete the experiment.
Note that this metric excludes I/O requests made to read and write file system
metadata, which is why it is the ``effective'' write throughput.

\subsubsection{Single File Writes}
In this experiment, we measure the time taken to append a varying number of
data blocks to a single file in testFS. Figure~\ref{fig:single-file-thpt}
compares the effective write throughput when asynchronous I/O is used and when
synchronous I/O is used. Figure~\ref{fig:single-file-speedup} summarizes this
comparison by illustrating the speedup as the number of blocks being appended
increases.

The speedup improves as the number of blocks being appended increases. This is
likely due to two reasons:
\begin{enumerate*}[label={(\roman*)}]
  \item a larger number of blocks allows the device to operate closer to its
    theoretical peak throughput, and
  \item the cost of reading and writing the file system metadata is amortized
    across a larger number of blocks.
\end{enumerate*}
Overall this experiment shows that our asynchronous write path can offer up to
a $1.5\times$ performance improvement for single file writes.

\subsubsection{Multi-file Writes}
In this experiment, we measure the time taken to append a fixed number of data
blocks to a varying number of files within a single transaction. For each trial
we open all the files (i.e. load the corresponding inodes), perform the writes
to each file, and then flush all of the dirty metadata blocks at once in bulk
(block freemap, checksum blocks, and inode blocks).
Figure~\ref{fig:multi-file-thpt} shows the effective write throughput when
appending 100 data blocks to a varying number of files.
Figure~\ref{fig:multi-file-speedup} shows the write speedup for two different
file sizes: 100 data blocks and 10 data blocks.

The speedup improves as the number of files being appended increases. Overall
as more files are written, more data blocks are being queued up to be written
to the underlying device. Additionally, the cost of performing metadata
operations is also amortized over all the file data blocks. Therefore the
reasoning for the improved speedup as the number of files increases is similar
to that of single file writes with increased data blocks. Overall this
experiment shows that our asynchronous write path can offer up to a $3\times$
performance improvement for multi-file writes.
