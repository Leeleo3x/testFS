\section{Evaluation}
Since testFS is a toy file system, we did not run any end-to-end file system
benchmarks such as {\tt fio}~\cite{fio}, or
Filebench~\cite{filebench-tarasov16}. As a result, we do not make claims about
performance improvements for real workloads. Instead, the goal of our
evaluation was to quantify the {\it potential} performance benefit of adopting
multi-threaded asynchronous I/O for NVMe devices in production-ready user space
file systems.

Overall our experiments show that our asynchronous I/O write path performs up
to $1.5\times$ better on single file writes and up to $3\times$ better on
multi-file writes compared to a synchronous I/O write path.

\subsection{Experimental Methodology}
\paragraph{Setup.}
We ran our experiments on a machine\footnote{We used {\tt mel-12}.} with a
14-core Intel Xeon 2.40 GHz CPU and 128 GB of memory. The machine was equipped
with two Seagate PLC Nytro 5000 NVMe SSDs. We used one of these SSDs, with a
capacity of 400 GB, in our experiments. Our code was linked with the version of
SPDK at git hash
\href{https://github.com/spdk/spdk/commit/a2bf3cded37b7cc7e402eae80da90891f921b56d}{\tt a2bf3cded}.
We used a fixed block size of 512 bytes when interacting with the SSD.

\paragraph{Methodology.}
In each experiment we measured the amount of time it took to complete a given
task up to a granularity of microseconds. We repeated each experiment five
times and used the average of all five trials in our results. We used primitive
futures (described in Section~\ref{sec:futures}) as the thread synchronization
mechanism in all of our experiments.

\paragraph{Baseline.}
We wanted to quantify the performance improvement associated with using
multi-threaded asynchronous I/O on the testFS file write path. Therefore
our baseline was a synchronous version of the testFS write path (each
asynchronous I/O function call was replaced with its synchronous counterpart).
For a fair comparison, both the synchronous and asynchronous write paths used
our improved write path implementation (as described in
Section~\ref{sec:writepath}).

\begin{figure}
  \centering
  \input{charts/raw-seq-speedup}
  \caption{The speedup of asynchronous I/O versus synchronous I/O for
    sequential reads and writes of a varying number of blocks.}
  \label{fig:raw-speedup}
\end{figure}

\subsection{Raw Sequential Reads and Writes}
To quantify the raw performance of asynchronous I/O versus synchronous I/O, we
ran two microbenchmarks that read and write a sequence of blocks one by one.
In each trial the benchmark reads (or writes) one block in a loop for a
pre-determined number of iterations. In the synchronous case each read/write
blocks until the request completes. In the asynchronous case we issue all
read/write requests and then wait for them to complete together in bulk.

Figure~\ref{fig:raw-speedup} shows our results. As the number of blocks read or
written increases, the speedup increases. Additionally the read speedup (up to
$7\times$) is greater than the write speedup (up to $2\times$). Reads are
faster than writes in flash-based SSDs, which could mean that they benefit more
from asynchronous I/O; this would explain the discrepancy in speedups between
raw reads and writes.

\begin{figure*}
\centering
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/single-file-write-speedup}
  \caption{Single file write speedup}\label{fig:single-file-speedup}
\end{subfigure}\hfill%
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/single-file-write-thpt}
  \caption{Single file write effective throughput}\label{fig:single-file-thpt}
\end{subfigure}
\par\bigskip
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/multi-file-write-speedup}
  \caption{Multi-file write speedup}\label{fig:multi-file-speedup}
\end{subfigure}%
\begin{subfigure}[t]{0.49\linewidth}
  \input{charts/multi-file-write-thpt}
  \caption{Multi-file write effective throughput for 100 data blocks}
  \label{fig:multi-file-thpt}
\end{subfigure}
\caption{The speedup and effective write throughput when appending a varying
number of data blocks to a single file and when appending a fixed number of
blocks to a varying number of files.}
\end{figure*}

\subsection{File Writes}
To quantify the performance improvements associated with a multi-threaded
asynchronous I/O write path, we ran experiments that varied the number of
blocks written to a file and the number of files written in a single
transaction. We report the overall speedup and the effective write throughput.

The effective write throughput is calculated by dividing the total size of the
data blocks being written by the time taken to complete the experiment. Note
that the effective write throughput excludes the size of the I/O requests made
to read and write file system metadata, but includes the time taken to execute
the requests.

\subsubsection{Single File Writes}
In this experiment we measured the time taken to append a varying number of
data blocks to a single file in testFS. Figure~\ref{fig:single-file-thpt} shows
the effective write throughput when multi-threaded asynchronous I/O is used and
when synchronous I/O is used. Figure~\ref{fig:single-file-speedup} summarizes
this comparison by illustrating the speedup as the number of blocks being
appended increases.

The speedup improves as the number of blocks being appended increases. We think
this is attributed to two reasons:
\begin{enumerate*}[label={(\roman*)}]
  \item a larger number of blocks being queued allows the device to operate
    closer to its peak throughput, and
  \item the cost of reading and writing the file system metadata is amortized
    across a larger number of blocks.
\end{enumerate*}
Overall this experiment shows that our multi-threaded asynchronous write path
can offer up to a $1.5\times$ performance improvement over synchronous I/O for
single file writes.

\subsubsection{Multi-file Writes}
In this experiment we measured the time taken to append a fixed number of data
blocks to a varying number of files within a single transaction. In each trial
we opened all the files (i.e. loaded the corresponding inodes), made the writes
to each file, and then flushed all of the dirty metadata blocks at once in bulk
(block freemap, checksum blocks, and inode blocks).
Figure~\ref{fig:multi-file-thpt} shows the effective write throughput when
100 data blocks were appended to a varying number of files.
Figure~\ref{fig:multi-file-speedup} shows the write speedup for two different
file sizes: 100 data blocks and 10 data blocks.

The speedup improves as the number of files being written to increases. Overall
as more files are written, more data blocks are being queued up to be written
to the underlying device. Therefore the reasoning for the trends is similar to
that of single file writes with increased data blocks. Overall this experiment
shows that our asynchronous write path can offer up to a $3\times$ performance
improvement for multi-file writes over synchronous I/O.
